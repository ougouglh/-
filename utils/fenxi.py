import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
import json
import os
from datetime import datetime

# å°è¯•å¯¼å…¥ tqdmï¼Œå¦‚æœæ²¡æœ‰åˆ™å¿½ç•¥
try:
    from tqdm import tqdm

    tqdm.pandas()
    HAS_TQDM = True
except ImportError:
    print("âš ï¸ æœªå®‰è£… tqdmï¼Œå°†ä¸æ˜¾ç¤ºè¿›åº¦æ¡ã€‚å®‰è£…å‘½ä»¤: pip install tqdm")
    HAS_TQDM = False

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class RepeatBuyerAnalyzer:
    """é‡å¤è´­ä¹°é¢„æµ‹æ•°æ®åˆ†æç±» - ä¿®å¤ç‰ˆæœ¬"""

    def __init__(self, train_path, user_info_path=None, user_log_path=None, output_dir='analysis_results'):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            train_path: è®­ç»ƒæ•°æ®è·¯å¾„
            user_info_path: ç”¨æˆ·ä¿¡æ¯æ•°æ®è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            user_log_path: ç”¨æˆ·è¡Œä¸ºæ—¥å¿—è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            output_dir: è¾“å‡ºç»“æœä¿å­˜ç›®å½•
        """
        self.train_path = train_path
        self.user_info_path = user_info_path
        self.user_log_path = user_log_path
        self.output_dir = output_dir

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self._create_output_dirs()

        # åŠ è½½æ•°æ®
        self.train_df = None
        self.user_info_df = None
        self.user_log_df = None

        self._load_data()

    def _create_output_dirs(self):
        """åˆ›å»ºè¾“å‡ºç›®å½•"""
        try:
            os.makedirs(self.output_dir, exist_ok=True)
            os.makedirs(os.path.join(self.output_dir, 'plots'), exist_ok=True)
            os.makedirs(os.path.join(self.output_dir, 'data'), exist_ok=True)
            print(f"âœ… è¾“å‡ºç›®å½•å·²åˆ›å»º: {self.output_dir}")
        except Exception as e:
            print(f"âš ï¸ åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {e}")
            self.output_dir = '.'  # ä½¿ç”¨å½“å‰ç›®å½•ä½œä¸ºå¤‡é€‰

    def _load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®æ–‡ä»¶"""
        print("=== å¼€å§‹åŠ è½½æ•°æ® ===")

        # åŠ è½½è®­ç»ƒæ•°æ®
        try:
            print("ğŸ“Š æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®...")
            self.train_df = pd.read_csv(self.train_path)
            print(f"âœ… è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸ: {self.train_df.shape}")

            # ä¿å­˜åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
            self._save_basic_stats()

        except Exception as e:
            print(f"âŒ è®­ç»ƒæ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise e

        # åŠ è½½ç”¨æˆ·ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        if self.user_info_path and os.path.exists(self.user_info_path):
            try:
                print("ğŸ‘¤ æ­£åœ¨åŠ è½½ç”¨æˆ·ä¿¡æ¯...")
                self.user_info_df = pd.read_csv(self.user_info_path)
                print(f"âœ… ç”¨æˆ·ä¿¡æ¯åŠ è½½æˆåŠŸ: {self.user_info_df.shape}")
            except Exception as e:
                print(f"âš ï¸ ç”¨æˆ·ä¿¡æ¯åŠ è½½å¤±è´¥: {e}")
        elif self.user_info_path:
            print(f"âš ï¸ ç”¨æˆ·ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {self.user_info_path}")

        # åŠ è½½ç”¨æˆ·è¡Œä¸ºæ—¥å¿—ï¼ˆå¯é€‰ï¼‰
        if self.user_log_path and os.path.exists(self.user_log_path):
            try:
                print("ğŸ“± æ­£åœ¨åŠ è½½ç”¨æˆ·è¡Œä¸ºæ—¥å¿—...")
                file_size = os.path.getsize(self.user_log_path)

                if file_size > 100 * 1024 * 1024 and HAS_TQDM:  # å¤§äº100MBä¸”æœ‰tqdm
                    print("â³ æ£€æµ‹åˆ°å¤§æ–‡ä»¶ï¼Œä½¿ç”¨åˆ†æ‰¹åŠ è½½...")
                    chunks = []
                    chunk_iter = pd.read_csv(self.user_log_path, chunksize=50000)
                    for chunk in tqdm(chunk_iter, desc="åŠ è½½æ•°æ®å—"):
                        chunks.append(chunk)
                    self.user_log_df = pd.concat(chunks, ignore_index=True)
                else:
                    self.user_log_df = pd.read_csv(self.user_log_path)
                print(f"âœ… ç”¨æˆ·è¡Œä¸ºæ—¥å¿—åŠ è½½æˆåŠŸ: {self.user_log_df.shape}")
            except Exception as e:
                print(f"âš ï¸ ç”¨æˆ·è¡Œä¸ºæ—¥å¿—åŠ è½½å¤±è´¥: {e}")
        elif self.user_log_path:
            print(f"âš ï¸ ç”¨æˆ·è¡Œä¸ºæ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {self.user_log_path}")

    def _save_basic_stats(self):
        """ä¿å­˜åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {
                'load_time': datetime.now().isoformat(),
                'train_shape': list(self.train_df.shape),
                'train_columns': list(self.train_df.columns),
                'train_dtypes': self.train_df.dtypes.astype(str).to_dict(),
                'train_memory_usage': int(self.train_df.memory_usage(deep=True).sum()),
            }

            stats_path = os.path.join(self.output_dir, 'basic_stats.json')
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ åŸºç¡€ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_path}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜åŸºç¡€ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

    def _save_plot(self, filename):
        """ä¿å­˜å½“å‰å›¾ç‰‡"""
        try:
            plot_path = os.path.join(self.output_dir, 'plots', f"{filename}.png")
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å›¾ç‰‡å·²ä¿å­˜: {plot_path}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")

    def _save_data(self, data, filename, data_type='json'):
        """ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶"""
        try:
            if data_type == 'json':
                file_path = os.path.join(self.output_dir, 'data', f"{filename}.json")
                if isinstance(data, pd.Series):
                    data_to_save = data.to_dict()
                elif isinstance(data, pd.DataFrame):
                    data_to_save = data.to_dict('records')
                else:
                    data_to_save = data

                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(data_to_save, f, ensure_ascii=False, indent=2, default=str)

            elif data_type == 'csv':
                file_path = os.path.join(self.output_dir, 'data', f"{filename}.csv")
                if isinstance(data, (pd.Series, pd.DataFrame)):
                    data.to_csv(file_path, index=True)
                else:
                    pd.DataFrame(data).to_csv(file_path, index=False)

            print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜: {file_path}")
            return file_path
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return None

    def basic_info_analysis(self):
        """åŸºç¡€ä¿¡æ¯åˆ†æ"""
        print("\n" + "=" * 50)
        print("ğŸ“Š åŸºç¡€æ•°æ®åˆ†æ")
        print("=" * 50)

        df = self.train_df

        # åŸºæœ¬ç»Ÿè®¡
        print("âœ… æ•°æ®åŸºæœ¬ä¿¡æ¯:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(df):,}")
        print(f"   å”¯ä¸€ç”¨æˆ·æ•°: {df['user_id'].nunique():,}")
        print(f"   å”¯ä¸€å•†å®¶æ•°: {df['merchant_id'].nunique():,}")
        print(f"   å¹³å‡æ¯ç”¨æˆ·å…³è”å•†å®¶æ•°: {len(df) / df['user_id'].nunique():.2f}")
        print(f"   å¹³å‡æ¯å•†å®¶å…³è”ç”¨æˆ·æ•°: {len(df) / df['merchant_id'].nunique():.2f}")

        # æ•°æ®è´¨é‡æ£€æŸ¥
        print("\nâœ… æ•°æ®è´¨é‡æ£€æŸ¥:")
        print(f"   ç¼ºå¤±å€¼æƒ…å†µ:")
        missing_info = df.isnull().sum()
        for col, missing_count in missing_info.items():
            print(f"     {col}: {missing_count} ({missing_count / len(df) * 100:.2f}%)")

        # é‡å¤å€¼æ£€æŸ¥
        duplicates = df.duplicated().sum()
        print(f"   é‡å¤è¡Œæ•°: {duplicates}")

        return df.describe()

    def label_distribution_analysis(self):
        """æ ‡ç­¾åˆ†å¸ƒåˆ†æ"""
        print("\n" + "=" * 50)
        print("ğŸ¯ æ ‡ç­¾åˆ†å¸ƒåˆ†æ")
        print("=" * 50)

        df = self.train_df

        # æ ‡ç­¾ç»Ÿè®¡
        print("â³ æ­£åœ¨åˆ†ææ ‡ç­¾åˆ†å¸ƒ...")
        label_counts = df['label'].value_counts().sort_index()
        total_labeled = label_counts.sum()

        print("âœ… æ ‡ç­¾åˆ†å¸ƒ:")
        for label, count in label_counts.items():
            percentage = count / total_labeled * 100
            print(f"   æ ‡ç­¾ {label}: {count:,} æ ·æœ¬ ({percentage:.2f}%)")

        # æ­£æ ·æœ¬æ¯”ä¾‹
        positive_rate = label_counts.get(1, 0) / total_labeled * 100
        print(f"\nğŸ¯ å…³é”®æŒ‡æ ‡:")
        print(f"   æ­£æ ·æœ¬æ¯”ä¾‹: {positive_rate:.2f}%")
        print(f"   è´Ÿæ ·æœ¬æ¯”ä¾‹: {100 - positive_rate:.2f}%")

        if positive_rate < 20:
            print("   âš ï¸ æ£€æµ‹åˆ°ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼")
            print("   ğŸ’¡ å»ºè®®ä½¿ç”¨: SMOTEã€ç±»åˆ«æƒé‡ã€é˜ˆå€¼ä¼˜åŒ–ç­‰æ–¹æ³•")

        # ä¿å­˜åˆ†æç»“æœ
        label_analysis_result = {
            'label_counts': {str(k): int(v) for k, v in label_counts.items()},
            'positive_rate': float(positive_rate),
            'negative_rate': float(100 - positive_rate),
            'is_imbalanced': positive_rate < 20,
            'analysis_time': datetime.now().isoformat()
        }
        self._save_data(label_analysis_result, 'label_distribution_analysis')

        # å¯è§†åŒ–
        plt.figure(figsize=(10, 4))

        plt.subplot(1, 2, 1)
        label_counts.plot(kind='bar', color=['#ff7f7f', '#7fbf7f'])
        plt.title('æ ‡ç­¾åˆ†å¸ƒ (ç»å¯¹æ•°é‡)')
        plt.xlabel('æ ‡ç­¾')
        plt.ylabel('æ ·æœ¬æ•°é‡')
        plt.xticks(rotation=0)

        plt.subplot(1, 2, 2)
        plt.pie(label_counts.values, labels=[f'æ ‡ç­¾{i}' for i in label_counts.index],
                autopct='%1.1f%%', colors=['#ff7f7f', '#7fbf7f'])
        plt.title('æ ‡ç­¾åˆ†å¸ƒ (ç™¾åˆ†æ¯”)')

        plt.tight_layout()
        self._save_plot('label_distribution')
        plt.show()

        return label_counts

    def user_behavior_analysis(self):
        """ç”¨æˆ·è¡Œä¸ºæ¨¡å¼åˆ†æ"""
        print("\n" + "=" * 50)
        print("ğŸ‘¤ ç”¨æˆ·è¡Œä¸ºæ¨¡å¼åˆ†æ")
        print("=" * 50)

        df = self.train_df

        # ç”¨æˆ·å…³è”å•†å®¶æ•°åˆ†æ
        print("â³ æ­£åœ¨åˆ†æç”¨æˆ·-å•†å®¶å…³è”æ¨¡å¼...")
        user_merchant_counts = df.groupby('user_id')['merchant_id'].count()

        print("âœ… ç”¨æˆ·å…³è”å•†å®¶æ•°åˆ†å¸ƒ:")
        merchant_count_dist = user_merchant_counts.value_counts().sort_index()
        total_users = len(user_merchant_counts)

        for count, users in merchant_count_dist.head(10).items():
            percentage = users / total_users * 100
            print(f"   å…³è”{count}ä¸ªå•†å®¶çš„ç”¨æˆ·: {users:,}ä¸ª ({percentage:.2f}%)")

        # ç”¨æˆ·å¿ è¯šåº¦åˆ†æ
        print("â³ æ­£åœ¨è®¡ç®—ç”¨æˆ·å¿ è¯šåº¦æŒ‡æ ‡...")
        single_merchant_users = merchant_count_dist.get(1, 0)
        multi_merchant_users = total_users - single_merchant_users

        print(f"\nğŸ¯ ç”¨æˆ·å¿ è¯šåº¦æ´å¯Ÿ:")
        print(f"   å•ä¸€å•†å®¶ç”¨æˆ·: {single_merchant_users:,} ({single_merchant_users / total_users * 100:.2f}%)")
        print(f"   å¤šå•†å®¶ç”¨æˆ·: {multi_merchant_users:,} ({multi_merchant_users / total_users * 100:.2f}%)")

        # å¤šå•†å®¶ç”¨æˆ·çš„é‡å¤è´­ä¹°ç‡
        print("â³ æ­£åœ¨åˆ†æä¸åŒç”¨æˆ·ç¾¤ä½“çš„é‡å¤è´­ä¹°ç‡...")
        multi_users = user_merchant_counts[user_merchant_counts > 1].index
        multi_user_repeat_rate = df[df['user_id'].isin(multi_users)]['label'].mean()
        single_user_repeat_rate = df[~df['user_id'].isin(multi_users)]['label'].mean()

        print(f"   å¤šå•†å®¶ç”¨æˆ·é‡å¤è´­ä¹°ç‡: {multi_user_repeat_rate * 100:.2f}%")
        print(f"   å•å•†å®¶ç”¨æˆ·é‡å¤è´­ä¹°ç‡: {single_user_repeat_rate * 100:.2f}%")

        # ä¿å­˜åˆ†æç»“æœ
        user_behavior_result = {
            'merchant_count_distribution': {str(k): int(v) for k, v in merchant_count_dist.head(20).items()},
            'total_users': int(total_users),
            'single_merchant_users': int(single_merchant_users),
            'multi_merchant_users': int(multi_merchant_users),
            'single_merchant_percentage': float(single_merchant_users / total_users * 100),
            'multi_merchant_percentage': float(multi_merchant_users / total_users * 100),
            'multi_user_repeat_rate': float(multi_user_repeat_rate),
            'single_user_repeat_rate': float(single_user_repeat_rate),
            'analysis_time': datetime.now().isoformat()
        }
        self._save_data(user_behavior_result, 'user_behavior_analysis')

        # ä¿å­˜è¯¦ç»†æ•°æ®
        self._save_data(user_merchant_counts, 'user_merchant_counts', 'csv')

        # å¯è§†åŒ–
        plt.figure(figsize=(12, 4))

        plt.subplot(1, 3, 1)
        merchant_count_dist.head(10).plot(kind='bar', color='skyblue')
        plt.title('ç”¨æˆ·å…³è”å•†å®¶æ•°åˆ†å¸ƒ')
        plt.xlabel('å…³è”å•†å®¶æ•°')
        plt.ylabel('ç”¨æˆ·æ•°é‡')

        plt.subplot(1, 3, 2)
        loyalty_data = [single_merchant_users, multi_merchant_users]
        loyalty_labels = ['å•ä¸€å•†å®¶', 'å¤šå•†å®¶']
        plt.pie(loyalty_data, labels=loyalty_labels, autopct='%1.1f%%',
                colors=['#ff9999', '#66b3ff'])
        plt.title('ç”¨æˆ·å¿ è¯šåº¦åˆ†å¸ƒ')

        plt.subplot(1, 3, 3)
        repeat_rates = [single_user_repeat_rate * 100, multi_user_repeat_rate * 100]
        plt.bar(loyalty_labels, repeat_rates, color=['#ff9999', '#66b3ff'])
        plt.title('ä¸åŒç±»å‹ç”¨æˆ·é‡å¤è´­ä¹°ç‡')
        plt.ylabel('é‡å¤è´­ä¹°ç‡ (%)')

        plt.tight_layout()
        self._save_plot('user_behavior_analysis')
        plt.show()

        return user_merchant_counts

    def quick_analysis(self):
        """å¿«é€Ÿåˆ†æ - ä»…åŒ…å«æ ¸å¿ƒåŠŸèƒ½"""
        print("ğŸš€ æ‰§è¡Œå¿«é€Ÿåˆ†æ...")

        # åŸºç¡€ä¿¡æ¯
        basic_stats = self.basic_info_analysis()

        # æ ‡ç­¾åˆ†å¸ƒ
        label_dist = self.label_distribution_analysis()

        # ç”¨æˆ·è¡Œä¸º
        user_behavior = self.user_behavior_analysis()

        # ç”Ÿæˆç®€è¦æŠ¥å‘Š
        total_samples = len(self.train_df)
        positive_rate = self.train_df['label'].mean() * 100
        single_merchant_rate = (user_behavior == 1).mean() * 100

        print("\n" + "=" * 60)
        print("ğŸ¯ å¿«é€Ÿåˆ†ææ€»ç»“")
        print("=" * 60)
        print(f"ğŸ“Š æ•°æ®è§„æ¨¡: {total_samples:,} æ ·æœ¬")
        print(f"âš–ï¸ æ­£æ ·æœ¬æ¯”ä¾‹: {positive_rate:.2f}%")
        print(f"ğŸ‘¤ å•ä¸€å•†å®¶ç”¨æˆ·æ¯”ä¾‹: {single_merchant_rate:.1f}%")

        if positive_rate < 10:
            print("âš ï¸ ä¸¥é‡ç±»åˆ«ä¸å¹³è¡¡ - éœ€è¦ç‰¹æ®Šå¤„ç†")

        # ä¿å­˜ç®€è¦æŠ¥å‘Š
        quick_report = {
            'total_samples': int(total_samples),
            'positive_rate': float(positive_rate),
            'single_merchant_rate': float(single_merchant_rate),
            'is_imbalanced': positive_rate < 20,
            'analysis_time': datetime.now().isoformat()
        }
        self._save_data(quick_report, 'quick_analysis_report')

        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")

        return {
            'basic_stats': basic_stats,
            'label_distribution': label_dist,
            'user_behavior': user_behavior,
            'quick_report': quick_report
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ•°æ®åˆ†æ...")

    # ä½¿ç”¨ä½ çš„æ–‡ä»¶è·¯å¾„
    analyzer = RepeatBuyerAnalyzer(
        train_path='../data/data_format1/train_format1.csv',
        user_info_path='../data/data_format1/user_info_format1.csv',
        user_log_path='../data/data_format1/user_log_format1.csv',
        output_dir='../outputs'
    )

    # æ‰§è¡Œå¿«é€Ÿåˆ†æï¼ˆæ¨èï¼‰
    results = analyzer.quick_analysis()

    print("\nğŸ‰ åˆ†æå®Œæˆï¼")
    print("ğŸ“ è¯·æŸ¥çœ‹ ../outputs ç›®å½•è·å–å®Œæ•´ç»“æœ")